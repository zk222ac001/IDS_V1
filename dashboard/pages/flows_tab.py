"""
IDS Flows Dashboard module for Streamlit

Drop this file into your dashboard package and import `render(flows_df, tab_container)`

Includes defensive fixes for:
 - Non-numeric packet_count (prevents slider crashes)
 - Stable, cached GeoIP enrichment with limits
 - Safe merges and DataFrame handling
 - Fixed pydeck usage and Streamlit component params
 - Stable cached helpers moved outside render()

Author: Generated by ChatGPT (GPT-5 Thinking mini)
"""

import logging
from hashlib import sha256
from typing import Optional

import pandas as pd
import pydeck as pdk
import streamlit as st
import geoip2.database
from streamlit_autorefresh import st_autorefresh
import os
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)
# Local helper (your project path)
from dashboard.utils.cleanup_db import cleanup_old_data

# --- Path-safe GeoIP configuration (DEPLOYMENT SAFE) ---------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, "..", ".."))
GEOIP_CITY_PATH = os.path.join(PROJECT_ROOT, "database", "GeoLite2-City.mmdb")
GEOIP_ASN_PATH  = os.path.join(PROJECT_ROOT, "database", "GeoLite2-ASN.mmdb")
# ----------------- Config / Constants -----------------
MAX_GEOIP_ENRICH = 400  # limit lookups to top N unique IPs to keep UI responsive
# ----------------- Cached helpers ---------------------
@st.cache_resource
def get_geoip_readers() -> Optional[dict]:
    """Return GeoIP readers or None if unavailable."""
    try:
        return {
            "city": geoip2.database.Reader(GEOIP_CITY_PATH),
            "asn": geoip2.database.Reader(GEOIP_ASN_PATH),
        }
    except Exception as e:
        log.exception("Failed to open GeoIP DBs: %s", e)
        return None


@st.cache_data(show_spinner=True)
def enrich_geo_data(unique_ips_tuple):
    """
    Enrich a tuple/list of IP strings with GeoIP city + ASN data.
    We limit processed IPs to MAX_GEOIP_ENRICH for responsiveness.

    Returns: pandas.DataFrame with columns [ip, lat, lon, country, asn]
    """
    readers = get_geoip_readers()
    if readers is None:
        log.warning("No GeoIP readers available")
        return pd.DataFrame(columns=["ip", "lat", "lon", "country", "asn"])  # empty schema

    unique_ips = list(unique_ips_tuple) if unique_ips_tuple is not None else []
    if not unique_ips:
        return pd.DataFrame(columns=["ip", "lat", "lon", "country", "asn"])  # empty

    # If too many IPs, choose deterministic top slice (sorted) to ensure stable cache keys
    if len(unique_ips) > MAX_GEOIP_ENRICH:
        unique_ips = sorted(unique_ips)[:MAX_GEOIP_ENRICH]

    geo_data = []
    for ip in unique_ips:
        try:
            city_resp = readers["city"].city(ip)
            asn_resp = readers["asn"].asn(ip)

            lat = getattr(city_resp.location, "latitude", None)
            lon = getattr(city_resp.location, "longitude", None)
            country = getattr(city_resp.country, "name", None)
            asn_org = getattr(asn_resp, "autonomous_system_organization", None)
            asn_num = getattr(asn_resp, "autonomous_system_number", None)

            if lat is not None and lon is not None:
                geo_data.append({
                    "ip": ip,
                    "lat": float(lat),
                    "lon": float(lon),
                    "country": country if country else "Unknown",
                    "asn": f"{asn_org or 'Unknown'} (AS{asn_num or 'N/A'})",
                })
        except Exception as e:
            # Keep this debug-level to avoid spamming logs in production
            log.debug("GeoIP failed for %s: %s", ip, e)

    return pd.DataFrame(geo_data)


def color_from_country(country: Optional[str]):
    """Deterministic RGB color per country string; fallback gray for missing."""
    if not isinstance(country, str) or country == "" or pd.isna(country):
        return [120, 120, 120]
    h = sha256(country.encode()).digest()
    return [h[0], h[1], h[2]]


def assign_risk_scores(series_packet_count: pd.Series) -> pd.Series:
    """Return categorical risk labels based on packet_count buckets."""
    s = pd.to_numeric(series_packet_count, errors="coerce").fillna(0).astype(int)
    return pd.cut(
        s,
        bins=[-1, 500, 1000, float("inf")],
        labels=["ğŸŸ¢ Low", "ğŸŸ  Medium", "ğŸ”´ High"],
    )


@st.cache_data
def get_csv_string(df: pd.DataFrame) -> str:
    csv_df = df.copy()
    if "timestamp" in csv_df.columns:
        csv_df["timestamp"] = pd.to_datetime(csv_df["timestamp"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
    return csv_df.to_csv(index=False)


# ----------------- Main render function -----------------
def render(flows_df: pd.DataFrame, tab_container) -> None:
    """
    Render the flows dashboard inside the provided Streamlit tab container.

    flows_df: DataFrame expected to contain at least columns:
      - packet_count, total_size, src_ip, dst_ip, protocol, timestamp

    This function is defensive: it creates missing columns with safe defaults and coerces types.
    """
    # Defensive copy
    df = flows_df.copy() if flows_df is not None else pd.DataFrame()

    # Ensure expected columns exist with safe defaults to avoid KeyErrors
    expected_cols = {
        "packet_count": 0,
        "total_size": 0,
        "src_ip": "",
        "dst_ip": "",
        "protocol": "UNKNOWN",
        "timestamp": pd.NaT,
    }
    for c, default in expected_cols.items():
        if c not in df.columns:
            df[c] = default

    # Enforce numeric types and sane defaults (prevents slider crashes and numeric ops)
    df["packet_count"] = pd.to_numeric(df["packet_count"], errors="coerce").fillna(0).astype(int)
    df["total_size"] = pd.to_numeric(df["total_size"], errors="coerce").fillna(0).astype(float)

    with tab_container:
        tab1, tab2, tab3 = st.tabs(["ğŸ“Š Flow Dashboard", "ğŸ—ºï¸ GeoIP Map", "ğŸ“ˆ Threat Graphs"])

        # ---------------- Sidebar filters ----------------
        with st.sidebar:
            st.header("ğŸ”§ Filters")
            query = st.text_input("ğŸ” Search", "", key="search_flows")
            protocols = ["All"] + sorted(df["protocol"].dropna().unique().tolist())
            selected_protocol = st.selectbox("ğŸ§­ Protocol", protocols, key="protocol_select")

            # slider bounds: ensure valid integers
            max_packets = int(df["packet_count"].max()) if not df.empty else 0
            max_packets = max(0, max_packets)
            slider_max = max(1, max_packets)  # slider requires max >= min
            min_packets = st.slider("ğŸ“Š Min Packets", 0, slider_max, 0, key="min_packets")

            refresh_toggle = st.checkbox("ğŸ” Auto-refresh every 30s", value=False, key="auto_refresh")

        if refresh_toggle:
            st_autorefresh(interval=30000, limit=None, key="refresh")

        # ---------------- Apply filters ----------------
        filtered = df.copy()
        if selected_protocol != "All":
            filtered = filtered[filtered["protocol"] == selected_protocol]

        filtered = filtered[filtered["packet_count"] >= int(min_packets)]

        if query:
            q = query.lower()
            mask = filtered.astype(str).apply(lambda col: col.str.lower().str.contains(q))
            filtered = filtered[mask.any(axis=1)]

        # parse timestamps if present
        if "timestamp" in filtered.columns:
            filtered["timestamp"] = pd.to_datetime(filtered["timestamp"], errors="coerce")

        # Risk & sizes
        filtered["Risk"] = assign_risk_scores(filtered["packet_count"])
        filtered["Bytes (MB)"] = filtered["total_size"] / (1024 * 1024)

        # ---------------- Geo enrichment (limit to unique top IPs to save time) ----------------
        unique_src_ips = filtered["src_ip"].dropna().astype(str).unique().tolist()
        if len(unique_src_ips) > 0:
            top_ips = (
                filtered.groupby("src_ip")["packet_count"].sum().sort_values(ascending=False).head(MAX_GEOIP_ENRICH).index.tolist()
            )
            geo_df = enrich_geo_data(tuple(top_ips))
        else:
            geo_df = pd.DataFrame(columns=["ip", "lat", "lon", "country", "asn"])

        # --------------- Sidebar: Auto-block countries ---------------
        with st.sidebar:
            st.markdown("### ğŸ”’ Auto-Block Countries")
            country_options = sorted(geo_df["country"].dropna().unique().tolist())
            blocked_countries = st.multiselect("Select Countries to Block", options=country_options, key="blocked_countries")
            blocked_ips = geo_df[geo_df["country"].isin(blocked_countries)]["ip"].tolist()
            if blocked_ips:
                filtered = filtered[~filtered["src_ip"].isin(blocked_ips)]
                st.warning(f"ğŸš« {len(blocked_ips)} IPs blocked from: {', '.join(blocked_countries)}")

            if not geo_df.empty:
                merge_cols = ["ip", "asn", "country"]
                filtered = filtered.merge(geo_df[merge_cols], left_on="src_ip", right_on="ip", how="left")
                filtered.rename(columns={"asn": "ASN", "country": "Country"}, inplace=True)
                if "ip" in filtered.columns:
                    filtered.drop(columns=["ip"], inplace=True)

        # --------------- Tab 1: Flow Dashboard ---------------
        with tab1:
            st.markdown("<h2 style='color:#650D61;'>ğŸ“¡ Network Flows Dashboard</h2>", unsafe_allow_html=True)

            if filtered.empty:
                st.info("ğŸš« No flow data available after filtering.")
                return

            col1, col2, col3, col4 = st.columns(4)
            col1.metric("ğŸŒ Total Flows", len(filtered))
            col2.metric("ğŸ“¦ Avg Packets", f"{filtered['packet_count'].mean():.2f}")
            col3.metric("ğŸ’¾ Avg Size (MB)", f"{filtered['Bytes (MB)'].mean():.2f}")
            col4.metric("ğŸŒ Unique IPs", filtered['src_ip'].nunique())

            display_df = filtered[["timestamp", "src_ip", "dst_ip", "protocol", "packet_count", "Bytes (MB)", "Risk", "ASN", "Country"]].copy()
            display_df["timestamp"] = display_df["timestamp"].dt.strftime("%Y-%m-%d %H:%M:%S")

            st.caption(f"ğŸ” Showing {len(display_df)} result(s)" + (f" for '{query}'" if query else ""))
            st.dataframe(
                display_df.sort_values("timestamp", ascending=False).head(200).style.bar(
                    subset=["packet_count", "Bytes (MB)"], color="#650D61"
                ),
                height=400,
            )

            csv_text = get_csv_string(display_df)
            st.download_button("ğŸ“¥ Download Filtered Flows", csv_text, file_name="flows.csv", mime="text/csv")

            if st.button("ğŸ§¹ Manually Run Cleanup (2+ Days Old)"):
                cleanup_old_data(2)
                st.cache_data.clear()
                st.success("Old data cleaned successfully!")

        # --------------- Tab 2: GeoIP Map ---------------
        with tab2:
            st.markdown("<h2 style='color:#650D61;'>ğŸ—ºï¸ Flow Source GeoIP Map</h2>", unsafe_allow_html=True)

            map_style = st.selectbox(
                "ğŸ—ºï¸ Select Map Style",
                options=[
                    "mapbox://styles/mapbox/light-v9",
                    "mapbox://styles/mapbox/dark-v10",
                    "mapbox://styles/mapbox/satellite-v9",
                ],
                format_func=lambda s: s.split("/")[-1].replace("-v9", "").capitalize(),
            )

            use_cluster = st.checkbox("ğŸ“Š Enable 3D Clustering", value=False, key="cluster")
            use_heatmap = st.checkbox("ğŸ”¥ Enable Heatmap View", value=False, key="heatmap")

            if geo_df.empty:
                st.warning("âŒ No valid GeoIP data for current flows.")
            else:
                if use_heatmap:
                    heatmap_layer = pdk.Layer(
                        "HeatmapLayer",
                        data=geo_df,
                        get_position='[lon, lat]',
                        aggregation='MEAN',
                        threshold=0.1,
                        intensity=1,
                    )
                    layers = [heatmap_layer]
                else:
                    if use_cluster:
                        layer = pdk.Layer(
                            "HexagonLayer",
                            data=geo_df,
                            get_position='[lon, lat]',
                            radius=100000,
                            elevation_scale=100,
                            elevation_range=[0, 3000],
                            extruded=True,
                            pickable=True,
                            coverage=1,
                        )
                    else:
                        geo_df["color"] = geo_df["country"].apply(color_from_country)
                        layer = pdk.Layer(
                            "ScatterplotLayer",
                            data=geo_df,
                            get_position='[lon, lat]',
                            get_color="color",
                            get_radius=60000,
                            pickable=True,
                        )
                    layers = [layer]

                tooltip = {
                    "html": "<b>IP:</b> {ip}<br><b>Country:</b> {country}<br><b>ASN:</b> {asn}",
                    "style": {"backgroundColor": "rgba(0,0,0,0.7)", "color": "white"},
                }

                st.pydeck_chart(
                    pdk.Deck(
                        map_style=map_style,
                        initial_view_state=pdk.ViewState(
                            latitude=20.0,
                            longitude=0.0,
                            zoom=1.5,
                            pitch=40 if use_cluster else 0,
                        ),
                        layers=layers,
                        tooltip=tooltip, # type: ignore[arg-type]
                    ),
                    use_container_width=True,
                )

        # --------------- Tab 3: Threat Graphs ---------------
        with tab3:
            st.markdown("<h2 style='color:#650D61;'>ğŸ“ˆ Threat Graphs</h2>", unsafe_allow_html=True)

            if filtered.empty:
                st.info("ğŸš« No Threat Graphs data available after filtering.")
                return

            # Flow timeline
            st.markdown("### â±ï¸ Flow Activity Over Time")
            if "timestamp" in filtered.columns and filtered["timestamp"].notna().any():
                timeline_df = (
                    filtered.groupby(filtered["timestamp"].dt.floor("min")).size().reset_index(name="Flow Count") # type: ignore
                )
                if not timeline_df.empty:
                    st.area_chart(timeline_df.rename(columns={"timestamp": "Time"}).set_index("Time"))
                else:
                    st.info("No flow timeline data available.")
            else:
                st.info("No timestamp data available for timeline.")

            # Threat by country
            st.markdown("### ğŸŒ Threat Intensity by Country")
            if "Country" in filtered.columns and filtered["Country"].notna().any():
                country_counts = filtered["Country"].fillna("Unknown").value_counts().reset_index()
                country_counts.columns = ["Country", "Flow Count"]
                st.bar_chart(country_counts.set_index("Country"))
            else:
                st.info("No country-level flow data available.")


# If run directly, show a small self-test (useful in dev). This will not run when imported.
if __name__ == "__main__":
    st.set_page_config(page_title="IDS Flows Dashboard - Dev", layout="wide")
    st.title("IDS Flows Dashboard - Dev Run")
    # create a tiny sample to sanity-check render
    sample = pd.DataFrame({
        "timestamp": [pd.Timestamp.now()],
        "src_ip": ["8.8.8.8"],
        "dst_ip": ["1.1.1.1"],
        "protocol": ["TCP"],
        "packet_count": [123],
        "total_size": [65536],
    })
    render(sample, st)
